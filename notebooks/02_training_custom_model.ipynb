{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom Cabruca Segmentation Model\n",
    "\n",
    "This notebook demonstrates how to train a custom segmentation model for your specific cabruca plantation data.\n",
    "\n",
    "## Contents\n",
    "1. Data Preparation\n",
    "2. Dataset Creation and Augmentation\n",
    "3. Model Configuration\n",
    "4. Training Loop with Monitoring\n",
    "5. Evaluation and Fine-tuning\n",
    "6. Export and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Add project source\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import project modules\n",
    "from models.cabruca_segmentation_model import CabrucaSegmentationModel\n",
    "from data.dataset import CabrucaDataset\n",
    "from training.advanced_trainer import AdvancedTrainer\n",
    "from evaluation.agroforestry_metrics import AgroforestryMetrics\n",
    "\n",
    "# Set device\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR = DATA_DIR / \"val\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    (dir_path / \"images\").mkdir(exist_ok=True)\n",
    "    (dir_path / \"masks\").mkdir(exist_ok=True)\n",
    "    (dir_path / \"annotations\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Data directories:\")\n",
    "print(f\"  Train: {TRAIN_DIR}\")\n",
    "print(f\"  Val: {VAL_DIR}\")\n",
    "print(f\"  Test: {TEST_DIR}\")\n",
    "\n",
    "# Check for existing data\n",
    "train_images = list((TRAIN_DIR / \"images\").glob(\"*\"))\n",
    "print(f\"\\nFound {len(train_images)} training images\")\n",
    "\n",
    "if len(train_images) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No training data found!\")\n",
    "    print(\"Please add your images and annotations to the data directories\")\n",
    "    print(\"\\nExpected structure:\")\n",
    "    print(\"  data/train/images/ - Training images\")\n",
    "    print(\"  data/train/masks/ - Segmentation masks\")\n",
    "    print(\"  data/train/annotations/ - COCO format annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Dataset (Optional)\n",
    "\n",
    "If you don't have real data yet, we'll create a synthetic dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(num_samples=10, img_size=512):\n",
    "    \"\"\"Create synthetic plantation images for demonstration.\"\"\"\n",
    "    import cv2\n",
    "    import json\n",
    "    \n",
    "    for split, num in [(\"train\", num_samples), (\"val\", num_samples//4), (\"test\", num_samples//4)]:\n",
    "        split_dir = DATA_DIR / split\n",
    "        \n",
    "        annotations = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": [\n",
    "                {\"id\": 1, \"name\": \"cacao\"},\n",
    "                {\"id\": 2, \"name\": \"shade\"}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for i in range(num):\n",
    "            # Create synthetic image\n",
    "            img = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "            img[:, :] = [34, 139, 34]  # Green background\n",
    "            \n",
    "            # Create semantic mask\n",
    "            mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "            mask[:, :] = 2  # Understory\n",
    "            \n",
    "            # Add random trees\n",
    "            np.random.seed(i)\n",
    "            num_trees = np.random.randint(10, 30)\n",
    "            \n",
    "            img_annotations = []\n",
    "            \n",
    "            for j in range(num_trees):\n",
    "                x = np.random.randint(50, img_size-50)\n",
    "                y = np.random.randint(50, img_size-50)\n",
    "                radius = np.random.randint(15, 40)\n",
    "                \n",
    "                # Determine species\n",
    "                is_cacao = np.random.random() > 0.3\n",
    "                \n",
    "                if is_cacao:\n",
    "                    color = [0, 100, 0]  # Dark green for cacao\n",
    "                    mask_value = 0  # Cacao in mask\n",
    "                    category_id = 1\n",
    "                else:\n",
    "                    color = [139, 69, 19]  # Brown for shade\n",
    "                    mask_value = 1  # Shade in mask\n",
    "                    category_id = 2\n",
    "                \n",
    "                # Draw tree in image\n",
    "                cv2.circle(img, (x, y), radius, color, -1)\n",
    "                cv2.circle(mask, (x, y), radius, mask_value, -1)\n",
    "                \n",
    "                # Add annotation\n",
    "                img_annotations.append({\n",
    "                    \"id\": len(annotations[\"annotations\"]),\n",
    "                    \"image_id\": i,\n",
    "                    \"category_id\": category_id,\n",
    "                    \"bbox\": [x-radius, y-radius, 2*radius, 2*radius],\n",
    "                    \"area\": np.pi * radius * radius,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "            \n",
    "            # Save image and mask\n",
    "            img_path = split_dir / \"images\" / f\"img_{i:04d}.jpg\"\n",
    "            mask_path = split_dir / \"masks\" / f\"mask_{i:04d}.png\"\n",
    "            \n",
    "            cv2.imwrite(str(img_path), img)\n",
    "            cv2.imwrite(str(mask_path), mask)\n",
    "            \n",
    "            # Add to annotations\n",
    "            annotations[\"images\"].append({\n",
    "                \"id\": i,\n",
    "                \"file_name\": img_path.name,\n",
    "                \"width\": img_size,\n",
    "                \"height\": img_size\n",
    "            })\n",
    "            annotations[\"annotations\"].extend(img_annotations)\n",
    "        \n",
    "        # Save annotations\n",
    "        ann_path = split_dir / \"annotations\" / \"annotations.json\"\n",
    "        with open(ann_path, 'w') as f:\n",
    "            json.dump(annotations, f, indent=2)\n",
    "        \n",
    "        print(f\"Created {num} synthetic {split} samples\")\n",
    "\n",
    "# Create synthetic dataset if no real data exists\n",
    "if len(train_images) == 0:\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    create_synthetic_dataset(num_samples=20)\n",
    "    print(\"‚úÖ Synthetic dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation transforms\n",
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(512, 512, scale=(0.8, 1.2)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CabrucaDataset(\n",
    "    image_dir=str(TRAIN_DIR / \"images\"),\n",
    "    mask_dir=str(TRAIN_DIR / \"masks\"),\n",
    "    annotation_file=str(TRAIN_DIR / \"annotations\" / \"annotations.json\"),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = CabrucaDataset(\n",
    "    image_dir=str(VAL_DIR / \"images\"),\n",
    "    mask_dir=str(VAL_DIR / \"masks\"),\n",
    "    annotation_file=str(VAL_DIR / \"annotations\" / \"annotations.json\"),\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4 if device.type == 'mps' else 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for macOS\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Visualize a batch\n",
    "batch = next(iter(train_loader))\n",
    "images, targets = batch\n",
    "\n",
    "print(f\"\\nBatch shape: {images.shape}\")\n",
    "print(f\"Number of targets: {len(targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    'num_instance_classes': 3,  # background, cacao, shade\n",
    "    'num_semantic_classes': 5,  # cacao, shade, understory, bare_soil, shadow\n",
    "    'backbone': 'resnet50',\n",
    "    'pretrained': True,\n",
    "    'freeze_backbone': False\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    'epochs': 10,  # Reduced for demo\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'optimizer': 'AdamW',\n",
    "    'scheduler': 'CosineAnnealingLR',\n",
    "    'gradient_clip': 1.0,\n",
    "    'mixed_precision': False,  # Set to False for MPS\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'early_stopping_patience': 5,\n",
    "    'save_best_only': True\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'model': model_config,\n",
    "    'training': training_config,\n",
    "    'data': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 0,\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset)\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = Path(\"../configs/notebook_config.yaml\")\n",
    "config_path.parent.mkdir(exist_ok=True)\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Configuration saved to:\", config_path)\n",
    "print(\"\\nModel Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "model = CabrucaSegmentationModel(\n",
    "    num_instance_classes=model_config['num_instance_classes'],\n",
    "    num_semantic_classes=model_config['num_semantic_classes'],\n",
    "    backbone=model_config['backbone'],\n",
    "    pretrained=model_config['pretrained']\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    config=training_config,\n",
    "    device=str(device),\n",
    "    output_dir=\"../outputs/notebook_training\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model and trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with live plotting\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Initialize metrics tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5  # Reduced for demo\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                   for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = trainer.train_step(images, targets)\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"): \n",
    "            images = images.to(device)\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                       for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss = trainer.validate_step(images, targets)\n",
    "            val_loss += loss\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Learning rate\n",
    "    current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if trainer.scheduler:\n",
    "        trainer.scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"../outputs/best_model.pth\")\n",
    "        print(f\"  üíæ Saved best model (val_loss: {avg_val_loss:.4f})\")\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1].plot(learning_rates, marker='o', color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training summary\n",
    "print(\"Training Summary:\")\n",
    "print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Final val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(val_losses):.4f} (Epoch {val_losses.index(min(val_losses))+1})\")\n",
    "print(f\"  Improvement: {(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"../outputs/best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on validation set\n",
    "from evaluation.agroforestry_metrics import AgroforestryMetrics\n",
    "\n",
    "evaluator = AgroforestryMetrics()\n",
    "all_metrics = []\n",
    "\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "        if i >= 5:  # Evaluate first 5 batches for demo\n",
    "            break\n",
    "            \n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate metrics for each image\n",
    "        for j in range(len(images)):\n",
    "            # Convert outputs to proper format\n",
    "            if isinstance(outputs, tuple):\n",
    "                instance_out, semantic_out = outputs\n",
    "            else:\n",
    "                instance_out = outputs\n",
    "                semantic_out = None\n",
    "            \n",
    "            # Simple metric calculation (placeholder)\n",
    "            # In real scenario, you would compute mAP, IoU, etc.\n",
    "            metrics = {\n",
    "                'num_detections': len(instance_out[j]['boxes']) if 'boxes' in instance_out[j] else 0,\n",
    "                'avg_confidence': np.mean(instance_out[j]['scores'].cpu().numpy()) if 'scores' in instance_out[j] and len(instance_out[j]['scores']) > 0 else 0\n",
    "            }\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "# Aggregate metrics\n",
    "if all_metrics:\n",
    "    avg_detections = np.mean([m['num_detections'] for m in all_metrics])\n",
    "    avg_confidence = np.mean([m['avg_confidence'] for m in all_metrics if m['avg_confidence'] > 0])\n",
    "    \n",
    "    print(f\"\\nValidation Metrics:\")\n",
    "    print(f\"  Average detections per image: {avg_detections:.1f}\")\n",
    "    print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image\n",
    "import cv2\n",
    "from inference.batch_inference import BatchInferenceEngine, VisualizationTools\n",
    "\n",
    "# Get a test image\n",
    "test_images = list((VAL_DIR / \"images\").glob(\"*\"))[:3]\n",
    "\n",
    "if test_images:\n",
    "    fig, axes = plt.subplots(len(test_images), 3, figsize=(12, 4*len(test_images)))\n",
    "    if len(test_images) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Initialize inference engine with trained model\n",
    "    engine = BatchInferenceEngine(\n",
    "        model_path=\"../outputs/best_model.pth\",\n",
    "        device=str(device)\n",
    "    )\n",
    "    \n",
    "    for idx, img_path in enumerate(test_images):\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        result = engine.process_single(str(img_path))\n",
    "        \n",
    "        # Original image\n",
    "        axes[idx, 0].imshow(img)\n",
    "        axes[idx, 0].set_title('Original')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Detection overlay\n",
    "        overlay = VisualizationTools.create_overlay(img, result, alpha=0.4)\n",
    "        axes[idx, 1].imshow(overlay)\n",
    "        axes[idx, 1].set_title(f'Detections ({len(result.trees)} trees)')\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        # Semantic segmentation\n",
    "        semantic_colored = BatchInferenceEngine.SEMANTIC_COLORS[result.semantic_map]\n",
    "        axes[idx, 2].imshow(semantic_colored.astype(np.uint8))\n",
    "        axes[idx, 2].set_title('Segmentation')\n",
    "        axes[idx, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions on Validation Set', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model checkpoint\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full checkpoint with metadata\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_config': training_config,\n",
    "    'epoch': len(train_losses),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "}\n",
    "\n",
    "checkpoint_path = OUTPUT_DIR / \"checkpoint_final.pth\"\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"‚úÖ Model checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Export to ONNX (optional)\n",
    "try:\n",
    "    dummy_input = torch.randn(1, 3, 512, 512).to(device)\n",
    "    onnx_path = OUTPUT_DIR / \"model.onnx\"\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model.instance_head,  # Export instance head only\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'},\n",
    "                     'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f\"‚úÖ ONNX model exported to {onnx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
    "\n",
    "# Create deployment package\n",
    "deployment_info = {\n",
    "    'model_path': str(checkpoint_path),\n",
    "    'model_config': model_config,\n",
    "    'input_size': [512, 512],\n",
    "    'normalization': {\n",
    "        'mean': [0.485, 0.456, 0.406],\n",
    "        'std': [0.229, 0.224, 0.225]\n",
    "    },\n",
    "    'classes': ['background', 'cacao', 'shade'],\n",
    "    'device_tested': str(device),\n",
    "    'performance': {\n",
    "        'val_loss': float(best_val_loss),\n",
    "        'training_time': 'see_logs',\n",
    "        'inference_time': 'measure_on_deployment'\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "deployment_path = OUTPUT_DIR / \"deployment_info.json\"\n",
    "with open(deployment_path, 'w') as f:\n",
    "    json.dump(deployment_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Deployment info saved to {deployment_path}\")\n",
    "print(\"\\nüì¶ Model ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "### Model Improvements\n",
    "1. **Data Augmentation**: Add more aggressive augmentations for better generalization\n",
    "2. **Hyperparameter Tuning**: Use Optuna or Ray Tune for systematic optimization\n",
    "3. **Architecture Changes**: Try different backbones (ResNet101, EfficientNet)\n",
    "4. **Loss Functions**: Experiment with focal loss, dice loss combinations\n",
    "\n",
    "### Deployment Options\n",
    "1. **API Server**: Deploy using FastAPI (see `api_server.py`)\n",
    "2. **Streamlit App**: Interactive web interface (see `viewer.py`)\n",
    "3. **Docker**: Containerize for cloud deployment\n",
    "4. **Edge Deployment**: Convert to TensorFlow Lite or Core ML\n",
    "\n",
    "### Performance Optimization\n",
    "1. **Model Pruning**: Reduce model size while maintaining accuracy\n",
    "2. **Quantization**: INT8 quantization for faster inference\n",
    "3. **Knowledge Distillation**: Train smaller student models\n",
    "4. **Batch Processing**: Optimize for throughput\n",
    "\n",
    "### Integration\n",
    "1. **QGIS Plugin**: Use the model in GIS workflows\n",
    "2. **Drone Integration**: Real-time processing of drone footage\n",
    "3. **Mobile App**: Deploy to mobile devices for field use\n",
    "\n",
    "### Resources\n",
    "- [Training Best Practices](../docs/training_guide.md)\n",
    "- [API Documentation](../API_DOCUMENTATION.md)\n",
    "- [Deployment Guide](../docs/deployment.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}